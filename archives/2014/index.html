<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Houliang | Coding and Learning</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="a software developer with great passion in Computer Science and Data Science.">
<meta property="og:type" content="website">
<meta property="og:title" content="Houliang | Coding and Learning">
<meta property="og:url" content="http://houlianglv.github.io/archives/2014/index.html">
<meta property="og:site_name" content="Houliang | Coding and Learning">
<meta property="og:description" content="a software developer with great passion in Computer Science and Data Science.">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Houliang | Coding and Learning">
<meta name="twitter:description" content="a software developer with great passion in Computer Science and Data Science.">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href='//fonts.useso.com/css?family=Open+Sans:400italic,400,600' rel='stylesheet' type='text/css'>
  <link href="//fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css" type="text/css">
  

</head>
<body>
  <div id="container">
    <header id="header">
  <div id="header-main" class="header-inner">
    <div class="outer">
      <a href="/" id="logo"><i class="logo"></i><span class="site-title">Houliang | Coding and Learning</span></a>
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        <div class="profile" id="profile-nav">
          <a id="profile-anchor" href="javascript:;"><img class="avatar" src="/css/images/photo.jpg"><i class="fa fa-caret-down"></i></a>
        </div>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"> </button><input type="hidden" name="q" value="site:http://houlianglv.github.io"></form>
      </div>
    </div>
  </div>
  <div id="main-nav-mobile" class="header-sub header-inner">
    <table class="menu outer">
      <tr>
      
        <td><a class="main-nav-link" href="/">Home</a></td>
      
        <td><a class="main-nav-link" href="/archives">Archives</a></td>
      
        <td><a class="main-nav-link" href="/about">About</a></td>
      
      <td>
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><input type="hidden" name="q" value="site:http://houlianglv.github.io"></form>
      </td>
      </tr>
    </table>
  </div>
  
  <div id="header-sub" class="header-sub header-inner">
    <div class="outer">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>
  
</header>
    <div class="outer">
      <aside id="profile">
  <div class="inner profile-inner">
  	<div class="base-info profile-block">
		  <img id="avatar" src="/css/images/photo.jpg">
      <h2 id="name">houlianglv</h2>
      <h3 id="title">Web Developer &amp; Machine Learning</h3>
      <span id="location"><i class="fa fa-map-marker"></i>Shanghai, China</span>
      <a id="follow" href="https://github.com/houlianglv">FOLLOW</a>
  	</div>
    <div class="article-info profile-block">
      <div class="article-info-block">
        5
        <span>posts</span>
      </div>
      <div class="article-info-block">
        13
        <span>tags</span>
      </div>
    </div>
    <div class="contact-info profile-block">
      <table class="contact-list">
        <tr>
        
          <td><a href="http://github.com/houlianglv" title="github"><i class="fa fa-github"></i></a></td>
        
          <td><a href="/#" title="twitter"><i class="fa fa-twitter"></i></a></td>
        
          <td><a href="/#" title="facebook"><i class="fa fa-facebook"></i></a></td>
        
          <td><a href="/#" title="dribbble"><i class="fa fa-dribbble"></i></a></td>
        
          <td><a href="/atom.xml" title="rss"><i class="fa fa-rss"></i></a></td>
        
        </tr>
      </table>
    </div>
  </div>
</aside>
      <section id="main">
  
  
    
    
      
        <div class="archive-year-wrap">
          <i class="fa fa-calendar"></i>
          <a href="/archives/2014" class="archive-year">2014</a>
        </div>
    
    <article id="post-intro-to-random-forest" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/12/02/intro-to-random-forest/">Intro to Random Forest</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2014/12/02/intro-to-random-forest/">
    <time datetime="2014-12-02T02:17:10.000Z" itemprop="datePublished">Dec 2 2014</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees’ habit of overfitting to their training set.</p>
<p>The algorithm for inducing a random forest was developed by Leo Breiman and Adele Cutler, and “Random Forests” is their trademark. The method combines Breiman’s “bagging” idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance.</p>
<p>The selection of a random subset of features is an example of the random subspace method, which, in Ho’s formulation, is a way to implement classification proposed by Eugene Kleinberg.</p>
<h2 id="History">History</h2><p>The early development of random forests was influenced by the work of Amit and Geman who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown, and variation among the trees is introduced by projecting the training data into a randomly chosen subspace before fitting each tree. Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Dietterich.<br>The introduction of random forests proper was first made in a paper by Leo Breiman. This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging. In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular:</p>
<ol>
<li>Using out-of-bag error as an estimate of the generalization error.</li>
<li>Measuring variable importance through permutation.</li>
</ol>
<p>The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.</p>
<h2 id="Algorithm">Algorithm</h2><h3 id="Preliminaries:_decision_tree_learning">Preliminaries: decision tree learning</h3><p>Decision trees are a popular method for various machine learning tasks. Tree learning “come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining”, say Hastie et al., because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate.<br>In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, because they have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.</p>
<h3 id="Tree_bagging">Tree bagging</h3><p>The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, …, xn with responses Y = y1, …, yn, bagging repeatedly selects a random sample with replacement of the training set and fits trees to these samples:<br>For b = 1, …, B:<br>Sample, with replacement, n training examples from X, Y; call these Xb, Yb.<br>Train a decision or regression tree fb on Xb, Yb.<br>After training, predictions for unseen samples x’ can be made by averaging the predictions from all the individual regression trees on x’:</p>
<img src="/2014/12/02/intro-to-random-forest/decision-formula.png">
<p>or by taking the majority vote in the case of decision trees.<br>This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.<br>The number of samples/trees, B, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees B can be found using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xᵢ, using only the trees that did not have xᵢ in their bootstrap sample. The training and test error tend to level off after some number of trees have been fit.</p>
<h3 id="From_bagging_to_random_forests">From bagging to random forests</h3><p>The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called “feature bagging”. The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated.<br>Typically, for a dataset with p features, √p features are used in each split.</p>
<h3 id="Extensions">Extensions</h3><p>Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. These are trained using bagging and the random subspace method, like in an ordinary random forest, but additionally the top-down splitting in the tree learner is randomized. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini coefficient), for each feature under consideration a random value is selected in the feature’s empirical range (in the tree’s training set, i.e., the bootstrap sample). The best of these is then chosen as the split.</p>
<h2 id="Usage">Usage</h2><p>We can use random forest as our prediction model with only several lines code in Python. Make sure that you have installed scikit-learn package. If you are ubuntu user(like me :) ), you can install the package in the command line:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install python-sklearn</span><br></pre></td></tr></table></figure>
<p>At this point, you can just run python under your folder, and type command as below to use random forest:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import the random forest package</span></span><br><span class="line">&gt;&gt;&gt; from sklearn.ensemble import RandomForestClassifier </span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the random forest object which will include all the parameters</span></span><br><span class="line"><span class="comment"># for the fit</span></span><br><span class="line">&gt;&gt;&gt; forest = RandomForestClassifier(n_estimators = <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit the training data to the Survived labels </span></span><br><span class="line"><span class="comment"># and create the decision trees</span></span><br><span class="line">&gt;&gt;&gt; forest = forest.fit(train_data[<span class="number">0</span>::,<span class="number">1</span>::],train_data[<span class="number">0</span>::,<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Take the same decision trees and run it on the test data</span></span><br><span class="line">&gt;&gt;&gt; output = forest.predict(<span class="built_in">test</span>_data)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://houlianglv.github.io/2014/12/02/intro-to-random-forest/" data-id="ci871l85n000068nr6guxthqa" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Algorithm/">Algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kaggle/">Kaggle</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    
    
    <article id="post-informartion-gain" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/07/informartion-gain/">Informartion Gain</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2014/11/07/informartion-gain/">
    <time datetime="2014-11-07T08:17:05.000Z" itemprop="datePublished">Nov 7 2014</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This intro is talking about a fundamental concept in data science and machine learning called “Information Gain”.</p>
<h2 id="Bits">Bits</h2><p>You are watching a set of independent randome samples of X</p>
<p>You see that X has four possible values<br>  P(X=A)=1/4  P(X=B)=1/4  P(X=C)=1/4  P(X=D)=1/4</p>
<p>So you might see :BAAACDBBADDAACC</p>
<p>You transmit data over a binary serial link, you can encode each rading with two bits.(e.g. A=00, B=01, C=10, D=11)</p>
<h2 id="Fewer_Bits">Fewer Bits</h2><p>Someone tells you that the probabilities are not equal<br>  P(X=A)=1/2  P(X=B)=1/4  P(X=C)=1/8  P(X=D)=1/8<br>…to invent a coding for your transmission that only uses 1.75 bits on average per symbol. How?<br>e.g. A=0, B=10, C=110, D=111<br>(This is just one of several ways)</p>
<h2 id="General_Case">General Case</h2><p>Suppose X can have one of m values: V1, V2, … Vm<br>P(X=V1)=p1, P(X=V2)=p2 … P(X=Vm)=pm<br>What’s the smallest possible number of bits, on average, per symbol, needed to transmit a stream of symbols drawn from X’s distribution? It’s:</p>
<img src="/2014/11/07/informartion-gain/bit-formula.png">
<p>H(X) = The entropy of X<br>• “High Entropy” means X is from a uniform (boring) distribution<br>• “Low Entropy” means X is from varied (peaks and valleys) distribution</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://houlianglv.github.io/2014/11/07/informartion-gain/" data-id="ci871l863000968nrxwho1vkj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Data-Mining/">Data Mining</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    
    
    <article id="post-decision-tree-tutorial" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/11/03/decision-tree-tutorial/">Decision Trees I</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2014/11/03/decision-tree-tutorial/">
    <time datetime="2014-11-03T03:05:30.000Z" itemprop="datePublished">Nov 3 2014</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="What_is_Classification?">What is Classification?</h2><ol>
<li><p>A major data mining operation.</p>
</li>
<li><p>Give one attribute of the data sample, try to predict the value of other attributes(or features)by means of some of the other available attributes. </p>
<p>Here are two types of categorical attribute:</p>
<p>• Categorical attribute: an attribute which takes on two or more discrete values. Also known as a symbolic attribute.</p>
<p>• Real attribute: a column of real numbers</p>
</li>
<li><p>Applies to categorical outputs.</p>
</li>
</ol>
<h2 id="Contingency_Tables">Contingency Tables</h2><p>We all know histogram. We can use histogram to have a straight intuition about our dataset. But a better name for histogram is<br>“A  One-dimensional Contingency Table”</p>
<p>Recipe for making a k-dimensional contingency table:</p>
<ol>
<li><p>Pick k attributes from your dataset. Call them a1,a2,… ak.</p>
</li>
<li><p>For every possible combination of values, a1’=x1’, a2’=x2’… ak’=xk’,record how frequently that combination occurs.<br>Fun fact: A database person would call this a “k-dimensional datacube”</p>
</li>
</ol>
<h2 id="On-Line_Analytical_Processing_(OLAP)">On-Line Analytical Processing (OLAP)</h2><p>• Software packages and database add-ons to do this are known as OLAP tools</p>
<p>• They usually include point and click navigation to view slices and aggregates of contingency tables</p>
<p>• They usually include nice histogram visualization</p>
<h2 id="Data_Mining">Data Mining</h2><p>• Data Mining is all about automating the process of searching for patterns in the data.</p>
<h3 id="Which_patterns_are_interesting?">Which patterns are interesting?</h3><p>Deciding whether a pattern is interesting</p>
<p>• We will use Information Theory</p>
<p>• A very large topic, originally used for compressing signals</p>
<p>• But more recently used for data mining…</p>
<h2 id="Learning_Decision_Tress">Learning Decision Tress</h2><p>• A Decision Tree is a tree-structured plan of a set of attributes to test in order to predict the output.</p>
<p>• To decide which attribute should be tested first, simply find the one with the highest information gain.</p>
<p>• Then recurse…</p>
<h3 id="Recursion_Step">Recursion Step</h3><p>Take the Original DatasetAnd partition it according to the value of the attribute we split on</p>
<h3 id="Base_Cases">Base Cases</h3><p>• Base Case One: If all records in current data subset have the same output then don’t recurse</p>
<p>• Base Case Two: If all records have exactly the same set of input attributes then don’t recurse</p>
<p>Perhaps we think that there is a Base Case 3: If all attributes have zero information gain then don’t recurse. Is it a good idea?</p>
<h3 id="The_Problem_with_Base_Case_3">The Problem with Base Case 3</h3><p>A simple example to disprove this propose is X OR Y = Z</p>
<h2 id="Basic_Decision_Tree_Building_Summarized">Basic Decision Tree Building Summarized</h2><p>Pseudo Code:</p>
<pre><code><span class="type">BuildTree</span>(<span class="type">DataSet</span>,<span class="type">Output</span>)
    <span class="type">If</span> all output values are the same <span class="keyword">in</span> <span class="type">DataSet</span>
        <span class="keyword">return</span> a leaf node that says “predict this unique output”
    <span class="type">If</span> all input values are the same
        <span class="keyword">return</span> a leaf node that says “predict the majority output”
    <span class="type">Else</span> 
        <span class="type">Find</span> attribute X <span class="keyword">with</span> highest <span class="type">Info</span> <span class="type">Gain</span> <span class="type">Suppose</span> X has nX <span class="keyword">distinct</span> values (i.e. X has arity nX).
        <span class="type">For</span> i’th child tree should be built by calling
            <span class="type">BuildTree</span>(<span class="type">DSi</span>,<span class="type">Output</span>) 
            <span class="type">Where</span> <span class="type">DSi</span> built consists <span class="keyword">of</span> all those records <span class="keyword">in</span> <span class="type">DataSet</span> <span class="keyword">for</span> which X = ith <span class="keyword">distinct</span> value <span class="keyword">of</span> X.
</code></pre><h2 id="Training_Set_Error">Training Set Error</h2><p>• For each record, follow the decision tree to see what it would predict<br>     For what number of records does the decision tree’s prediction disagree with the true value in the database?</p>
<p>• This quantity is called the training set error. The smaller the better.</p>
<h3 id="Why_are_we_doing_this_learning_anyway?">Why are we doing this learning anyway?</h3><p>• It is not usually in order to predict the training data’s output on data we have already seen.</p>
<p>• It is more commonly in order to predict the output value for future data we have not yet seen.</p>
<p> Warning: A common data mining misperception is that the above two bullets are the only possible reasons for learning. There are at least a dozen others.</p>
<h2 id="Test_Set_Error">Test Set Error</h2><p>• Suppose we are forward thinking.</p>
<p>• We hide some data away when we learn the decision tree.</p>
<p>• But once learned, we see how well the tree predicts that data.</p>
<p>• This is a good simulation of what happens when we try to predict future data.</p>
<p>• And it is called Test Set Error.</p>
<h2 id="Overfitting">Overfitting</h2><p>• Definition: If your machine learning algorithm fits noise (i.e. pays attention to parts of the data that are irrelevant) it is overfitting.<br>• Fact (theoretical and empirical): If your machine learning algorithm is overfitting then it may perform less well on test set data.</p>
<h2 id="How_to_avoid_overfitting?">How to avoid overfitting?</h2><p>• Usually we do not know in advance which are the irrelevant variables<br>• …and it may depend on the context<br>        For example, if y = a AND b then b is an irrelevant variable only in the portion of the tree in which a=0 But we can use simple statistics to warn us that we might be overfitting.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://houlianglv.github.io/2014/11/03/decision-tree-tutorial/" data-id="ci871l869000m68nrq70ktts5" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithm/">algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/classification/">classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/decision-trees/">decision trees</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/information-gain/">information gain</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    
    
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/08/02/hello-world/">Hexo Tutorial</a>
    </h1>
  

        <div class="article-meta">
          <div class="article-date">
  <i class="fa fa-calendar"></i>
  <a href="/2014/08/02/hello-world/">
    <time datetime="2014-08-02T11:17:10.000Z" itemprop="datePublished">Aug 2 2014</time>
  </a>
</div>
          
  <div class="article-category">
  	<i class="fa fa-folder"></i>
    <a class="article-category-link" href="/categories/Hexo/">Hexo</a>
  </div>

        </div>
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="http://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="http://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick_Start">Quick Start</h2><h3 id="Create_a_new_post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run_server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate_static_files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy_to_remote_sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="http://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://houlianglv.github.io/2014/08/02/hello-world/" data-id="ci871l867000f68nr8r4o4n18" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tutorial/">Tutorial</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
      
        <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">recents</h3>
    <div class="widget">
      <ul id="recent-post">
        
          <li>
            <div class="item-thumbnail">
              <a href="/2015/02/01/algorithm-stanford-week4-1/" class="thumbnail">
  
    <span style="background-image:url(/2015/02/01/algorithm-stanford-week4-1/scc-example-image.png
)" alt="Algorithm-Stanford-Week4" class="thumbnail-image"></span>
  
</a>
            </div>
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/Algorithm/">Algorithm</a></p>
              <p class="item-title"><a href="/2015/02/01/algorithm-stanford-week4-1/" class="title">Algorithm-Stanford-Week4</a></p>
              <p class="item-date"><time datetime="2015-02-01T09:41:04.000Z" itemprop="datePublished">Feb 1 2015</time></p>
            </div>
          </li>
        
          <li>
            <div class="item-thumbnail">
              <a href="/2014/12/02/intro-to-random-forest/" class="thumbnail">
  
    <span style="background-image:url(/2014/12/02/intro-to-random-forest/decision-formula.png
)" alt="Intro to Random Forest" class="thumbnail-image"></span>
  
</a>
            </div>
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a></p>
              <p class="item-title"><a href="/2014/12/02/intro-to-random-forest/" class="title">Intro to Random Forest</a></p>
              <p class="item-date"><time datetime="2014-12-02T02:17:10.000Z" itemprop="datePublished">Dec 2 2014</time></p>
            </div>
          </li>
        
          <li>
            <div class="item-thumbnail">
              <a href="/2014/11/07/informartion-gain/" class="thumbnail">
  
    <span style="background-image:url(/2014/11/07/informartion-gain/bit-formula.png
)" alt="Informartion Gain" class="thumbnail-image"></span>
  
</a>
            </div>
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a></p>
              <p class="item-title"><a href="/2014/11/07/informartion-gain/" class="title">Informartion Gain</a></p>
              <p class="item-date"><time datetime="2014-11-07T08:17:05.000Z" itemprop="datePublished">Nov 7 2014</time></p>
            </div>
          </li>
        
          <li>
            <div class="item-thumbnail">
              <a href="/2014/11/03/decision-tree-tutorial/" class="thumbnail">
  
    <span class="thumbnail-image thumbnail-none"></span>
  
</a>
            </div>
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a></p>
              <p class="item-title"><a href="/2014/11/03/decision-tree-tutorial/" class="title">Decision Trees I</a></p>
              <p class="item-date"><time datetime="2014-11-03T03:05:30.000Z" itemprop="datePublished">Nov 3 2014</time></p>
            </div>
          </li>
        
          <li>
            <div class="item-thumbnail">
              <a href="/2014/08/02/hello-world/" class="thumbnail">
  
    <span class="thumbnail-image thumbnail-none"></span>
  
</a>
            </div>
            <div class="item-inner">
              <p class="item-category"><a class="article-category-link" href="/categories/Hexo/">Hexo</a></p>
              <p class="item-title"><a href="/2014/08/02/hello-world/" class="title">Hexo Tutorial</a></p>
              <p class="item-date"><time datetime="2014-08-02T11:17:10.000Z" itemprop="datePublished">Aug 2 2014</time></p>
            </div>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hexo/">Hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a><span class="category-list-count">3</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Mining/">Data Mining</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kaggle/">Kaggle</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tutorial/">Tutorial</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/classification/">classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/coursera/">coursera</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/data-structure/">data structure</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/decision-trees/">decision trees</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/information-gain/">information gain</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">tag cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a><a href="/tags/Data-Mining/" style="font-size: 10px;">Data Mining</a><a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a><a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a><a href="/tags/Machine-Learning/" style="font-size: 20px;">Machine Learning</a><a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a><a href="/tags/algorithm/" style="font-size: 20px;">algorithm</a><a href="/tags/classification/" style="font-size: 10px;">classification</a><a href="/tags/coursera/" style="font-size: 10px;">coursera</a><a href="/tags/data-structure/" style="font-size: 10px;">data structure</a><a href="/tags/decision-trees/" style="font-size: 10px;">decision trees</a><a href="/tags/information-gain/" style="font-size: 10px;">information gain</a><a href="/tags/machine-learning/" style="font-size: 10px;">machine learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/02/">February 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/12/">December 2014</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/11/">November 2014</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2014/08/">August 2014</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
  <div id="toTop" class="fa fa-chevron-up"></div>
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 houlianglv<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>.
    </div>
  </div>
</footer>
    


<script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

  </div>
</body>
</html>
